# 과대적합과 과소적합

- 머신러닝 모델에서 자주 발생하는 문제
- 과대적합: 모델이 훈련 데이터에서는 우수 but, 새로운 데이터 제대로 예측 x
![title](https://www.datarobot.com/wp-content/uploads/2018/03/Screen-Shot-2018-03-22-at-11.22.15-AM-e1527613915658.png)   

- 과소적합: 훈련 데이터, 새로운 데이터 둘 다  성능 😣

### 성능 저하
### 모델 선택 실패
모델을 변경해 문제를 완화할 수 있음.
- 과대적합 -> 모델 구조가 너무 복잡해 훈련 데이터에만 의존하게 되어 성능이 저하됨.
- 과소적합 -> 모델 구조가 너무 단순해 데이터의 특징 제대로 학습하지 못한 경우
### bias-varience trade-off
![title](https://the-examples-book.com/starter-guides/data-science/_images/bias_variance_tradeoff.png)   

낮은 편향 & 낮은 분산 -> 우수한 성능!
- 높은 분산 -> 추청치에 대한 변동 폭 커짐, 데이터가 갖고 있는 노이즈까지 학습에 포함됨 -> 과대적합!
- 높은 편향 -> 항상 일정한 값을 갖게 될 확률 커짐 -> 데이터의 특징을 제대로 학습 x -> 과소적합!
- but, 편향과 분산은 서로 반비례 관계
### 과대적합과 과소적합 문제 해결
- 데이터 수집
학습 데이터 수를 늘림으로써 일반적인 규칙을 더욱 잘 찾을 수 있게 한다.

🙄 학습 데이터를 늘리는 것이 상황에 상관없이 항상 학습의 성능을 향상시키는가?
- 과소적합일 경우 먼저 학습량을 늘려봐야한다! 그 후 새로운 데이터 투입.
- 과대적합일 경우 학습량을 늘리는 것은 도움 되지 않고, 새로운 데이터를 쓰는 것이 필요. 또는 가중치를 빼는 기법인 drop out을 활용해봐야함.(layer를 끊었다가 붙였다가 해보는 작업)

- 피처 엔지니어링(데이터 변환)
추가적인 데이터 수집이 어려울 경우 기존 학습 데이터에서 변수나 특징을 추출하거나 피처를 더 작은 차원으로 축소하여 강건한 모델을 만든다.


# 배치 정규화
- 내부 공변량 변화를 줄여 과대적합 방지
- 내부 공변량 변화가 발생하면 은닉층에서 다음 은닉층으로 전달될 때 입력값이 균일해지지 않아 가중치가 제대로 갱신 x ->그럼 학습이 불안정해지고 가중치가 일정한 값으로 수렴하기 어렵... 
#### 배치 정규화 방식
[100, 1, 1], [1, 0.01, 0.01]
= [1.4242, -0.7071, -0.7071]
- 배치 정규화를 적용하면 입력이 일반화되고 독립적으로 정규화가 수행되기에 더 빠르게 값을 수렴!
- 입력이 정규화되므로 초기 가중치에 대한 영향 ⬇
- 이미지 분류 모델에 적용시 14배 더 적은 학습으로도 동일한 정확도 달성! ㄷㄷㄷ

# 가중치 초기화
- 모델의 초기 가중치 값(모델 최적화에 완전완전 중요!!!) 설정
- 적절한 초기값 -> 기울기 폭주/ 소실 문제 완화
- 모델 수렴 속도 향상

1. 상수 초기화
- 가장 간단한 방법! but, 일반적으로 사용 x (모든 가중치 초깃값을 같은 값으로 초기화하면 배열 구조의 가중치에서는 문제 발생)
- 초기 가중치 값을 모두 같은 값으로 초기화

e.g.> 0, 1, 특정값, 단위행렬, 디랙 델타 함수 값 등
![title](https://ballpen.blog/wp-content/uploads/2024/01/%E1%84%83%E1%85%A6%E1%86%AF%E1%84%90%E1%85%A1-%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE-1-1536x661.jpg)   

240723 딥러닝 스터디 질문
1. 수진 언니

과대적합를 막기 위해 사용하는 방법 중 하나인 조기중단: 성능을 계속 평가를 하다가 저하되기 전에 중지하는 학습 방법인데: 이게 뒤에 데이터를 어떻게 하는지?
-> epoch 수에 따라서 달라지는 듯...?ㅠㅠ

2. 제인언니

제로샷 전이 학습 원리?
-> "카테고리 단위로 학습"을 진행하기에 이게 가능한 것 같음.
주변 단어들의 문맥 등등을 파악하는 느낌

3. 희원언니

"렐루함수의 죽은 뉴런 문제?"
"컷믹스": 어떤 포인트들을 이 모델이 중요하게 생각하는지를 알 수 있지 않을까..?
예- 귀 뾰족 or 하관 모양

4. 다솔

중국: 인공지능 강국, 가장 큰 이유는 데이터가 많아서
(9천개->upscaling) vs (6만개->downscaling)
6만개쪽이 결과가 좋았움..
두개의 데이터를 비교하는 거였는데
